{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습에 필요한 파이썬 라이브러리를 불러옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyTorch 관련 된 라이브러리.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # optimization에 관한 모듈.\n",
    "import torchvision # 이미지 관련 전처리, pretrained된 모델, 데이터 로딩에 관한 패키지입니다.\n",
    "from torch.autograd import Variable # 미분자동화가 된 변수 설정을 위한 모듈\n",
    "from torchvision.utils import save_image # 이미지 저장을 위한 torchvision의 모듈\n",
    "import torchvision.datasets as vision_dsets\n",
    "import torchvision.transforms as T # 이미지 전처리 모듈입니다.\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils import data\n",
    "\n",
    "# 기타 필요한 라이브러리.\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import easydict # ipynb에서는 다루기 귀찮은 argparse를 대체해주는 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버전을 한 번 확인해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version :  3.7.1 (default, Dec 14 2018, 19:28:38) \n",
      "[GCC 7.3.0]\n",
      "numpy version :  1.15.4\n",
      "matplotlib version : 3.0.2\n",
      "pytorch version :  1.0.1.post2\n",
      "torchvision version :  0.2.2\n",
      "Cuda :  True\n"
     ]
    }
   ],
   "source": [
    "print('python version : ', sys.version)\n",
    "print('numpy version : ', np.version.version)\n",
    "print('matplotlib version :', mpl.__version__)\n",
    "print('pytorch version : ', torch.__version__)\n",
    "print('torchvision version : ', torchvision.__version__)\n",
    "print('Cuda : ', torch.cuda.is_available()) # GPU 세팅이 제대로 됬는지 확인해 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarGAN\n",
    "\n",
    "#### StarGAN은 multi-domain의 image translation이 가능하다는 것에 가장 큰 contribution을 갖는다.\n",
    "\n",
    "![구조](https://github.com/yunjey/stargan/blob/master/jpg/model.jpg?raw=true)\n",
    "\n",
    "---\n",
    "\n",
    "#### 위 구조를 실제 dataset 상황에서 보자면 아래와 같다.\n",
    "\n",
    "\n",
    "\n",
    "![구조2](https://github.com/yunjey/stargan/blob/master/jpg/model2.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA Dataset Download "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!bash download.sh celeba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "CelebA의 경우에는 pytorch에서 기본으로 제공하는 data loader가 없기에 조금 복잡한 data loader를 구현해줘야 합니다. dataloader의 경우는 각각의 dataset들마다 코딩해줘야하는 방향이 천차만별입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA(data.Dataset):\n",
    "    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n",
    "        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.attr_path = attr_path\n",
    "        self.selected_attrs = selected_attrs\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.train_dataset = []\n",
    "        self.test_dataset = []\n",
    "        self.attr2idx = {}\n",
    "        self.idx2attr = {}\n",
    "        self.preprocess()\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.num_images = len(self.train_dataset)\n",
    "        else:\n",
    "            self.num_images = len(self.test_dataset)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n",
    "        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n",
    "        all_attr_names = lines[1].split()\n",
    "        for i, attr_name in enumerate(all_attr_names):\n",
    "            self.attr2idx[attr_name] = i\n",
    "            self.idx2attr[i] = attr_name\n",
    "\n",
    "        lines = lines[2:]\n",
    "        random.seed(1234)\n",
    "        random.shuffle(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            split = line.split()\n",
    "            filename = split[0]\n",
    "            values = split[1:]\n",
    "\n",
    "            label = []\n",
    "            for attr_name in self.selected_attrs:\n",
    "                idx = self.attr2idx[attr_name]\n",
    "                label.append(values[idx] == '1')\n",
    "\n",
    "            if (i+1) < 2000:\n",
    "                self.test_dataset.append([filename, label])\n",
    "            else:\n",
    "                self.train_dataset.append([filename, label])\n",
    "\n",
    "        print('Finished preprocessing the CelebA dataset...')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
    "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
    "        filename, label = dataset[index]\n",
    "        image = Image.open(os.path.join(self.image_dir, filename))\n",
    "        return self.transform(image), torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images.\"\"\"\n",
    "        return self.num_images\n",
    "    \n",
    "def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n",
    "               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    transform = []\n",
    "    if mode == 'train':\n",
    "        transform.append(T.RandomHorizontalFlip())\n",
    "    transform.append(T.CenterCrop(crop_size))\n",
    "    transform.append(T.Resize(image_size))\n",
    "    transform.append(T.ToTensor())\n",
    "    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "    transform = T.Compose(transform)\n",
    "\n",
    "    dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n",
    "\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=(mode=='train'),\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "### ResidualBlock\n",
    "\n",
    "Residual Block 이라는 개념은 2015년 Imagenet Challenge에서 ResNet이라는 모델이 우승을 차지하며 그 효과를 입증받은 모델이다.\n",
    "\n",
    "![resnet](https://neurohive.io/wp-content/uploads/2019/01/resnet-e1548261477164.png)\n",
    "\n",
    "### Instance Normalization\n",
    "\n",
    "Batch Normalization의 경우에는 모델이 batch크기의 영향을 크게 받고 특히 batch 크기가 1처럼 작은 경우 variance가 너무 작아져 noisy한 결과를 갖게 될 수 있습니다. 이러한 batch size에 영향을 받는 batch norm의 한계를 넘기 위해 많은 normalization 방법론들이 나왔고, 그 중 하나가 Instance Normalization으로서 image to image translation에서 자주 사용되는 방법입니다.\n",
    "\n",
    "![IN](https://bloglunit.files.wordpress.com/2018/04/ec8aa4ed81aceba6b0ec83b7-2018-04-11-ec98a4ed9b84-3-07-41.png?w=1400)\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "![BN equation](https://i.stack.imgur.com/VDqKY.jpg)\n",
    "\n",
    "#### Instance Normalization\n",
    "\n",
    "![IN equation](https://i.stack.imgur.com/X5z48.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=?, padding=?, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=?, padding=?, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ?????\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3+?????, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-sampling layers.\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "        # 이 for 문이 끝난 뒤의 curr dim = ???????\n",
    "\n",
    "        # Bottleneck layers.\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(?????????????????????????????????????????)\n",
    "\n",
    "        # Up-sampling layers.\n",
    "        for i in range(2):\n",
    "            layers.append(nn.??????????????(curr_dim, ?????, kernel_size=?, stride=?, padding=?, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(???????, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = ???????\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Replicate spatially and concatenate domain information.\n",
    "        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n",
    "        # This is because instance normalization ignores the shifting (or bias) effect.\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.???([x, c], dim=1)\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.01))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        kernel_size = ??????????????????????????????????????\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = nn.Conv2d(curr_dim, ?, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(curr_dim, ???, kernel_size=kernel_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_src = self.conv1(h)\n",
    "        out_cls = self.conv2(h)\n",
    "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration, hyper parameter\n",
    "\n",
    "필요한 여러가지 설정들과 hyper parameter들을 정의해준다. 이렇게 설정들을 따로 정해두는 것은 다른 데이터셋의 학습과 같은 상황에서 유연하게 대처할 수 있도록 코드를 모듈화해두는 작업이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "    \n",
    "    # Model configuration\n",
    "    \"c_dim\": 5, # dimension of domain labels\n",
    "    \"celeba_crop_size\": 178, # crop size for the CelebA dataset\n",
    "    \"image_size\": 128, # image resolution\n",
    "    \"g_conv_dim\": 64, # number of conv filters in the first layer of G\n",
    "    \"d_conv_dim\": 64, # number of conv filters in the first layer of D\n",
    "    \"g_repeat_num\": 6, # number of residual blocks in G\n",
    "    \"d_repeat_num\": 6, # number of strided conv layers in D\n",
    "    \"lambda_cls\": 1.0, # weight for domain classification loss\n",
    "    \"lambda_rec\": 10.0, # weight for reconstruction loss\n",
    "    \"lambda_gp\": 10.0, # weight for gradient penalty\n",
    "    \n",
    "    # Training configuration\n",
    "#     \"dataset\": 'CelebA'\n",
    "    \"batch_size\": 16, # mini-batch size\n",
    "    \"num_iters\": 200000, # number of total iterations for training D\n",
    "    \"num_iters_decay\": 100000, # number of iterations for decaying lr\n",
    "    \"g_lr\": 0.0001, # learning rate for G\n",
    "    \"d_lr\": 0.0001, # learning rate for D\n",
    "    \"n_critic\": 5, # number of D updates per each G update\n",
    "    \"beta1\": 0.5, # beta1 for Adam optimizer\n",
    "    \"beta2\": 0.999, # beta2 for Adam optimizer\n",
    "    \"resume_iters\": None, # resume training from this step\n",
    "    \"selected_attrs\": ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'], # selected attributes for the CelebA dataset\n",
    "    \n",
    "    # Test configuration\n",
    "    \"test_iters\": 200000, # test model from this step\n",
    "    \n",
    "    # Miscellaneous\n",
    "    \"num_workers\": 1, #\n",
    "    \"mode\": 'train', # train or test mode\n",
    "    \n",
    "    # Directories\n",
    "    \"celeba_image_dir\": 'data/celeba/images', #\n",
    "    \"attr_path\": 'data/celeba/list_attr_celeba.txt', #\n",
    "    \n",
    "#     \"log_dir\": 'stargan_celeba/logs', #\n",
    "    \"model_save_dir\": 'stargan_celeba/models', #\n",
    "    \"sample_dir\": 'stargan_celeba/samples', #\n",
    "    \"result_dir\": 'stargan_celeba/results', #\n",
    "    \n",
    "    # Step size\n",
    "    \"log_step\": 10, #\n",
    "    \"sample_step\": 1000, #\n",
    "    \"model_save_step\": 10000, #\n",
    "    \"lr_update_step\": 1000 #\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config.model_save_dir):\n",
    "    os.makedirs(config.model_save_dir)\n",
    "if not os.path.exists(config.sample_dir):\n",
    "    os.makedirs(config.sample_dir)\n",
    "if not os.path.exists(config.result_dir):\n",
    "    os.makedirs(config.result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "#### Adversarial Loss\n",
    "\n",
    "![loss](https://drive.google.com/uc?export=view&id=1eOBGMlNiw2Obn6nHiVYYRFYi3zrTFD4X)\n",
    "\n",
    "#### Domain Classification Loss\n",
    "\n",
    "흔히 사용되는 classification loss (binary cross entropy)\n",
    "\n",
    "#### Reconstruction Loss\n",
    "\n",
    "![rec loss](https://drive.google.com/uc?export=view&id=1hntL27V8kJSdmBkOIN3JJ5bJu8k6pRUO)\n",
    "\n",
    "#### Full Objective\n",
    "![loss2](https://drive.google.com/uc?export=view&id=1LYq5233lnCxISttXtINlLkgV0voHvNiC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    \"\"\"Solver for training and testing StarGAN.\"\"\"\n",
    "\n",
    "    def __init__(self, celeba_loader, config):\n",
    "        \"\"\"Initialize configurations.\"\"\"\n",
    "\n",
    "        # Data loader.\n",
    "        self.celeba_loader = celeba_loader\n",
    "\n",
    "        # Model configurations.\n",
    "        self.c_dim = config.c_dim\n",
    "        self.image_size = config.image_size\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.g_repeat_num = config.g_repeat_num\n",
    "        self.d_repeat_num = config.d_repeat_num\n",
    "        self.lambda_cls = config.lambda_cls\n",
    "        self.lambda_rec = config.lambda_rec\n",
    "        self.lambda_gp = config.lambda_gp\n",
    "\n",
    "        # Training configurations.\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_iters = config.num_iters\n",
    "        self.num_iters_decay = config.num_iters_decay\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.n_critic = config.n_critic\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.resume_iters = config.resume_iters\n",
    "        self.selected_attrs = config.selected_attrs\n",
    "\n",
    "        # Test configurations.\n",
    "        self.test_iters = config.test_iters\n",
    "\n",
    "        # Miscellaneous.\n",
    "        self.device = torch.device(????? if torch.cuda.is_available() else ?????)\n",
    "\n",
    "        # Directories.\n",
    "#         self.log_dir = config.log_dir\n",
    "        self.sample_dir = config.sample_dir\n",
    "        self.model_save_dir = config.model_save_dir\n",
    "        self.result_dir = config.result_dir\n",
    "\n",
    "        # Step size.\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.model_save_step = config.model_save_step\n",
    "        self.lr_update_step = config.lr_update_step\n",
    "\n",
    "        # Build the model and tensorboard.\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Create a generator and a discriminator.\"\"\"\n",
    "        self.G = Generator(???????????????????????????????????)\n",
    "        self.D = Discriminator(??????????????????????????????????????????????????) \n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
    "        self.print_network(self.G, 'G')\n",
    "        self.print_network(self.D, 'D')\n",
    "            \n",
    "        self.G.to(self.device)\n",
    "        self.D.to(self.device)\n",
    "        \n",
    "    def print_network(self, model, name):\n",
    "        \"\"\"Print out the network information.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(model)\n",
    "        print(name)\n",
    "        print(\"The number of parameters: {}\".format(num_params))\n",
    "        \n",
    "    def restore_model(self, resume_iters):\n",
    "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
    "        print('Loading the trained models from step {}...'.format(resume_iters))\n",
    "        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
    "        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
    "        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
    "        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "    def update_lr(self, g_lr, d_lr):\n",
    "        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n",
    "        for param_group in self.g_optimizer.param_groups:\n",
    "            param_group['lr'] = g_lr\n",
    "        for param_group in self.d_optimizer.param_groups:\n",
    "            ???????????????????????\n",
    "            \n",
    "    def reset_grad(self):\n",
    "        \"\"\"Reset the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "        ???????????????????????????\n",
    "            \n",
    "    def denorm(self, x):\n",
    "        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp_(0, 1)\n",
    "    \n",
    "    def gradient_penalty(self, y, x):\n",
    "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "        weight = torch.ones(y.size()).to(self.device)\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "        return torch.mean((dydx_l2norm-1)**2)\n",
    "    \n",
    "    def label2onehot(self, labels, dim):\n",
    "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        out = torch.zeros(batch_size, dim)\n",
    "        out[np.arange(batch_size), labels.long()] = 1\n",
    "        return out\n",
    "    \n",
    "    def create_labels(self, c_org, c_dim=5, selected_attrs=None):\n",
    "        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n",
    "        # Get hair color indices.\n",
    "        hair_color_indices = []\n",
    "        for i, attr_name in enumerate(selected_attrs):\n",
    "            if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n",
    "                hair_color_indices.append(i)\n",
    "\n",
    "        c_trg_list = []\n",
    "        for i in range(c_dim):\n",
    "            c_trg = c_org.clone()\n",
    "            if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n",
    "                c_trg[:, i] = 1\n",
    "                for j in hair_color_indices:\n",
    "                    if j != i:\n",
    "                        c_trg[:, j] = 0\n",
    "            else:\n",
    "                c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
    "\n",
    "            c_trg_list.append(c_trg.to(self.device))\n",
    "        return c_trg_list\n",
    "    \n",
    "    def classification_loss(self, logit, target):\n",
    "        \"\"\"Compute binary or softmax cross entropy loss.\"\"\"\n",
    "        return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train StarGAN within a CelebA dataset.\"\"\"\n",
    "        # Set data loader.\n",
    "        data_loader = self.celeba_loader\n",
    "\n",
    "        # Fetch fixed inputs for debugging.\n",
    "        data_iter = iter(data_loader)\n",
    "        x_fixed, c_org = next(data_iter)\n",
    "        x_fixed = x_fixed.to(self.device)\n",
    "        c_fixed_list = self.create_labels(c_org, self.c_dim, self.selected_attrs)\n",
    "\n",
    "        # Learning rate cache for decaying.\n",
    "        g_lr = self.g_lr\n",
    "        d_lr = self.d_lr\n",
    "\n",
    "        # Start training from scratch or resume training.\n",
    "        start_iters = 0\n",
    "        if self.resume_iters:\n",
    "            start_iters = self.resume_iters\n",
    "            self.restore_model(self.resume_iters)\n",
    "\n",
    "        # Start training.\n",
    "        print('Start training...')\n",
    "        start_time = time.time()\n",
    "        for i in range(start_iters, self.num_iters):\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                             1. Preprocess input data                                #\n",
    "            # =================================================================================== #\n",
    "\n",
    "            # Fetch real images and labels.\n",
    "            try:\n",
    "                x_real, label_org = next(data_iter)\n",
    "            except:\n",
    "                ?????????????????????????????\n",
    "                ??????????????????????????????????\n",
    "\n",
    "            # Generate target domain labels randomly.\n",
    "            rand_idx = torch.randperm(label_org.size(0))\n",
    "            label_trg = label_org[rand_idx]\n",
    "\n",
    "            \n",
    "            c_org = label_org.clone()\n",
    "            c_trg = label_trg.clone()\n",
    "\n",
    "            x_real = x_real.to(???????????)           # Input images.\n",
    "            c_org = c_org.to(???????????)             # Original domain labels.\n",
    "            c_trg = c_trg.to(???????????)             # Target domain labels.\n",
    "            label_org = label_org.to(???????????)     # Labels for computing classification loss.\n",
    "            label_trg = label_trg.to(???????????)     # Labels for computing classification loss.\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                             2. Train the discriminator                              #\n",
    "            # =================================================================================== #\n",
    "\n",
    "            # Compute loss with real images.\n",
    "            out_src, out_cls = ????????????\n",
    "            d_loss_real = - torch.mean(out_src)\n",
    "            d_loss_cls = ??????????????????????????????????\n",
    "\n",
    "            # Compute loss with fake images.\n",
    "            x_fake = ?????????????????\n",
    "            out_src, out_cls = self.D(???????.detach())\n",
    "            d_loss_fake = ??????????????????\n",
    "\n",
    "            # Compute loss for gradient penalty.\n",
    "            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n",
    "            x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n",
    "            out_src, _ = self.D(x_hat)\n",
    "            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n",
    "\n",
    "            # Backward and optimize.\n",
    "            d_loss = ???????????????????????????????????????????????? + self.lambda_gp * d_loss_gp\n",
    "            self.reset_grad()\n",
    "            d_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "\n",
    "            # Logging.\n",
    "            loss = {}\n",
    "            loss['D/loss_real'] = d_loss_real.item()\n",
    "            loss['D/loss_fake'] = d_loss_fake.item()\n",
    "            loss['D/loss_cls'] = d_loss_cls.item()\n",
    "            loss['D/loss_gp'] = d_loss_gp.item()\n",
    "            \n",
    "            # =================================================================================== #\n",
    "            #                               3. Train the generator                                #\n",
    "            # =================================================================================== #\n",
    "            \n",
    "            if (i+1) % self.n_critic == 0:\n",
    "                # Original-to-target domain.\n",
    "                x_fake = ?????????????????\n",
    "                out_src, out_cls = self.D(x_fake)\n",
    "                g_loss_fake = ???????????????\n",
    "                g_loss_cls = ????????????????????????????\n",
    "\n",
    "                # Target-to-original domain.\n",
    "                x_reconst = ??????????????????\n",
    "                g_loss_rec = ???????????????????????????????????\n",
    "\n",
    "                # Backward and optimize.\n",
    "                g_loss = ???????????????????????????????????????????????????????????\n",
    "                ?????????????????\n",
    "                ?????????????????\n",
    "                ?????????????????????????\n",
    "\n",
    "                # Logging.\n",
    "                loss['G/loss_fake'] = g_loss_fake.item()\n",
    "                loss['G/loss_rec'] = g_loss_rec.item()\n",
    "                loss['G/loss_cls'] = g_loss_cls.item()\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                                 4. Miscellaneous                                    #\n",
    "            # =================================================================================== #\n",
    "\n",
    "            # Print out training information.\n",
    "            if (i+1) % self.log_step == 0:\n",
    "                et = time.time() - start_time\n",
    "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
    "                for tag, value in loss.items():\n",
    "                    log += \", {}: {:.4f}\".format(tag, value)\n",
    "                print(log)\n",
    "\n",
    "                \n",
    "\n",
    "            # Translate fixed images for debugging.\n",
    "            if (i+1) % self.sample_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    x_fake_list = [x_fixed]\n",
    "                    for c_fixed in c_fixed_list:\n",
    "                        x_fake_list.append(self.G(x_fixed, c_fixed))\n",
    "                    x_concat = torch.cat(x_fake_list, dim=3)\n",
    "                    sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n",
    "                    save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
    "                    print('Saved real and fake images into {}...'.format(sample_path))\n",
    "\n",
    "            # Save model checkpoints.\n",
    "            if (i+1) % self.model_save_step == 0:\n",
    "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
    "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
    "                torch.save(self.G.state_dict(), G_path)\n",
    "                torch.save(self.D.state_dict(), D_path)\n",
    "                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
    "\n",
    "            # Decay learning rates.\n",
    "            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
    "                g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
    "                d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
    "                self.update_lr(g_lr, d_lr)\n",
    "                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
    "                \n",
    "    def test(self):\n",
    "        \"\"\"Translate images using StarGAN trained on a single dataset.\"\"\"\n",
    "        # Load the trained generator.\n",
    "        self.restore_model(self.test_iters)\n",
    "        \n",
    "        # Set data loader.\n",
    "        data_loader = self.celeba_loader\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_real, c_org) in enumerate(data_loader):\n",
    "\n",
    "                # Prepare input images and target domain labels.\n",
    "                x_real = ????????????????????\n",
    "                c_trg_list = ????????????????????????????????????????????????\n",
    "\n",
    "                # Translate images.\n",
    "                x_fake_list = [x_real]\n",
    "                for c_trg in c_trg_list:\n",
    "                    x_fake_list.append(self.G(x_real, c_trg))\n",
    "\n",
    "                # Save the translated images.\n",
    "                x_concat = torch.cat(x_fake_list, dim=3)\n",
    "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
    "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
    "                print('Saved real and fake images into {}...'.format(result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(8, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace)\n",
      "    (18): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (22): Tanh()\n",
      "  )\n",
      ")\n",
      "G\n",
      "The number of parameters: 8430528\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(2048, 5, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      ")\n",
      "D\n",
      "The number of parameters: 44762048\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunsung/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed [0:00:04], Iteration [10/200000], D/loss_real: -24.0860, D/loss_fake: 5.0567, D/loss_cls: 4.2179, D/loss_gp: 0.0656, G/loss_fake: -3.8394, G/loss_rec: 0.5249, G/loss_cls: 3.0116\n",
      "Elapsed [0:00:05], Iteration [20/200000], D/loss_real: -38.6063, D/loss_fake: 6.1324, D/loss_cls: 4.6678, D/loss_gp: 0.7411, G/loss_fake: -4.1855, G/loss_rec: 0.5080, G/loss_cls: 2.8512\n",
      "Elapsed [0:00:07], Iteration [30/200000], D/loss_real: -30.2640, D/loss_fake: 8.8760, D/loss_cls: 3.9003, D/loss_gp: 0.2229, G/loss_fake: -9.5772, G/loss_rec: 0.4262, G/loss_cls: 2.8190\n",
      "Elapsed [0:00:09], Iteration [40/200000], D/loss_real: -17.4891, D/loss_fake: 3.3436, D/loss_cls: 3.6712, D/loss_gp: 0.1782, G/loss_fake: 1.3596, G/loss_rec: 0.3969, G/loss_cls: 3.1906\n",
      "Elapsed [0:00:11], Iteration [50/200000], D/loss_real: -27.2563, D/loss_fake: 15.7733, D/loss_cls: 3.4486, D/loss_gp: 0.3871, G/loss_fake: -7.6021, G/loss_rec: 0.3785, G/loss_cls: 3.2814\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-da676c9350d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-418af4cb66cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# Logging.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G/loss_fake'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G/loss_rec'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G/loss_cls'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.backends import cudnn\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,\n",
    "                           config.celeba_crop_size, config.image_size, config.batch_size,\n",
    "                           'CelebA', config.mode, config.num_workers)\n",
    "\n",
    "solver = Solver(celeba_loader, config)\n",
    "\n",
    "if config.mode == 'train':\n",
    "    ?????????????\n",
    "elif config.mode == 'test':\n",
    "    ?????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Weight Download\n",
    "\n",
    "StarGAN의 경우 multi domain의 상황에서 그 contribution이 잘 드러나는 모델이므로, 이 전 실습들과 같이 mnist와 같은 작은 데이터에서만 진행할 수가 없다. 모델 자체의 iteration 수도 크기 때문에 실습 시간이라는 제한 내에서 결과를 보기 어렵다. 따라서 이번 실습에서는 위의 train code의 진행 가능 여부를 확인했으니, prtrained weight를 통해 진행하도록 한다.\n",
    "\n",
    "Pretrained Weight는 보통 pt, pth, ckpt 등의 확장자를 가지고 있는 파일로, 이전에 트레이닝 해둔 모델의 weight 값들을 모델의 텐서 형태에 맞추어 저장하고 있다.\n",
    "\n",
    "이미 트레이닝된 모델을 활용하는 것은 다 방면에서 활용 가능하다. (pretrained model을 통한 transfer learning으로의 활용이 많으며, 이는 딥러닝의 대부분의 분야에서 활용된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!bash download.sh pretrained-celeba-128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "    \n",
    "    # Model configuration\n",
    "    \"c_dim\": 5, # dimension of domain labels\n",
    "    \"celeba_crop_size\": 178, # crop size for the CelebA dataset\n",
    "    \"image_size\": 128, # image resolution\n",
    "    \"g_conv_dim\": 64, # number of conv filters in the first layer of G\n",
    "    \"d_conv_dim\": 64, # number of conv filters in the first layer of D\n",
    "    \"g_repeat_num\": 6, # number of residual blocks in G\n",
    "    \"d_repeat_num\": 6, # number of strided conv layers in D\n",
    "    \"lambda_cls\": 1.0, # weight for domain classification loss\n",
    "    \"lambda_rec\": 10.0, # weight for reconstruction loss\n",
    "    \"lambda_gp\": 10.0, # weight for gradient penalty\n",
    "    \n",
    "    # Training configuration\n",
    "#     \"dataset\": 'CelebA'\n",
    "    \"batch_size\": 16, # mini-batch size\n",
    "    \"num_iters\": 200000, # number of total iterations for training D\n",
    "    \"num_iters_decay\": 100000, # number of iterations for decaying lr\n",
    "    \"g_lr\": 0.0001, # learning rate for G\n",
    "    \"d_lr\": 0.0001, # learning rate for D\n",
    "    \"n_critic\": 5, # number of D updates per each G update\n",
    "    \"beta1\": 0.5, # beta1 for Adam optimizer\n",
    "    \"beta2\": 0.999, # beta2 for Adam optimizer\n",
    "    \"resume_iters\": None, # resume training from this step\n",
    "    \"selected_attrs\": ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'], # selected attributes for the CelebA dataset\n",
    "    \n",
    "    # Test configuration\n",
    "    \"test_iters\": 200000, # test model from this step\n",
    "    \n",
    "    # Miscellaneous\n",
    "    \"num_workers\": 1, #\n",
    "    \"mode\": 'test', # train or test mode\n",
    "    \n",
    "    # Directories\n",
    "    \"celeba_image_dir\": 'data/celeba/images', #\n",
    "    \"attr_path\": 'data/celeba/list_attr_celeba.txt', #\n",
    "    \n",
    "#     \"log_dir\": 'stargan_celeba/logs', #\n",
    "    \"model_save_dir\": 'stargan_celeba/models', #\n",
    "    \"sample_dir\": 'stargan_celeba/samples', #\n",
    "    \"result_dir\": 'stargan_celeba/results', #\n",
    "    \n",
    "    # Step size\n",
    "    \"log_step\": 10, #\n",
    "    \"sample_step\": 1000, #\n",
    "    \"model_save_step\": 10000, #\n",
    "    \"lr_update_step\": 1000 #\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(8, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace)\n",
      "    (18): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (22): Tanh()\n",
      "  )\n",
      ")\n",
      "G\n",
      "The number of parameters: 8430528\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(2048, 5, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      ")\n",
      "D\n",
      "The number of parameters: 44762048\n",
      "Loading the trained models from step 200000...\n",
      "Saved real and fake images into stargan_celeba/results/1-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/2-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/3-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/4-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/5-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/6-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/7-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/8-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/9-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/10-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/11-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/12-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/13-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/14-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/15-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/16-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/17-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/18-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/19-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/20-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/21-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/22-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/23-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/24-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/25-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/26-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/27-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/28-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/29-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/30-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/31-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/32-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/33-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/34-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/35-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/36-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/37-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/38-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/39-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/40-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/41-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/42-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/43-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/44-images.jpg...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved real and fake images into stargan_celeba/results/45-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/46-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/47-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/48-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/49-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/50-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/51-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/52-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/53-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/54-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/55-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/56-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/57-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/58-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/59-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/60-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/61-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/62-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/63-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/64-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/65-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/66-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/67-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/68-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/69-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/70-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/71-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/72-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/73-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/74-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/75-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/76-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/77-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/78-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/79-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/80-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/81-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/82-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/83-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/84-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/85-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/86-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/87-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/88-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/89-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/90-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/91-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/92-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/93-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/94-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/95-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/96-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/97-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/98-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/99-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/100-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/101-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/102-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/103-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/104-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/105-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/106-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/107-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/108-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/109-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/110-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/111-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/112-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/113-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/114-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/115-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/116-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/117-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/118-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/119-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/120-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/121-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/122-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/123-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/124-images.jpg...\n",
      "Saved real and fake images into stargan_celeba/results/125-images.jpg...\n"
     ]
    }
   ],
   "source": [
    "from torch.backends import cudnn\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,\n",
    "                           config.celeba_crop_size, config.image_size, config.batch_size,\n",
    "                           'CelebA', config.mode, config.num_workers)\n",
    "\n",
    "solver = Solver(celeba_loader, config)\n",
    "\n",
    "if config.mode == 'train':\n",
    "    solver.train()\n",
    "elif config.mode == 'test':\n",
    "    solver.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
