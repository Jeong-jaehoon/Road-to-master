{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning and Activation function\n",
    "\n",
    "Cost function / objective function   \n",
    "**평가의 기준이 무엇이냐 그게 위의 fucntion이 되는것**  \n",
    "**MSE 라고하는 것**   \n",
    "**cross entropy**  \n",
    "**Gradient descent**\n",
    "**Backpropagation** : 그냥 사용할줄 아는것만으로는 부족하다 개념을 이해해야한다.\n",
    "\n",
    "학습을 할때는 메모리양을 많이 먹어야한다.   \n",
    "\n",
    "시그모이드 function activation 알고리즘중 하나인데 \n",
    "\n",
    "*sigmoid 3 problems*\n",
    "\n",
    "1. Saturated neurons \"ill\" the gradients  : 평평한 부분이 많다\n",
    "2. Sigmoid outputs are not zero-centerd : 양수값 밖에 존재하지않는다. (zig-zag path)\n",
    "3. exp() is a bit compute expensive : 익스포넨셜의 계산 량 문제\n",
    "\n",
    "결국 sigmoid를 사용하지 않게된다..\n",
    "\n",
    "gradient 가 0 이나온다 = 더이상 업데이트가 되지않을 것이다.\n",
    "\n",
    "\n",
    "**tanh : sigamoid의 형제**\n",
    "1. Squashes numbers to range [ -1, 1]\n",
    "2. zero centered( nice)\n",
    "3. still kills gradients when saturated (bad) \n",
    "\n",
    "- gradient vasnishing 급격한 기울기 변화 때문에 \n",
    "\n",
    "\n",
    "**ReLu**\n",
    "gradient vanishing 을 막을 수 있다.\n",
    "깊은 모델을 학습할수있는 Relu\n",
    "1. Does not saturate\n",
    "2. Very computationally efficient\n",
    "3. Converges much faster than sigmoid/tanh in practice (e.g 6x)\n",
    "4. Actually more biologically plausible than sigmoid\n",
    "\n",
    "*단점 *\n",
    "\n",
    "Not zero-centered output  \n",
    "An annoyance\n",
    "\n",
    "Relu 의 집합으로 함수를 모방! 이미지 참조\n",
    "\n",
    "Relu 시간은 오래걸리지만 성능은 tanh보다 올라갈수 있음  \n",
    "tanh는 반대로 시간은 적게걸리지만 시간은 적게 걸림\n",
    "\n",
    "ReLu의 단점   \n",
    "\n",
    "Dead ReLu  :  0에서 업데이트가 절대 안됨 운이 나쁘게 weight가 전부 0 이 되는경우 output이 항상 0이 되는거에요  \n",
    "이경우 아무리 학습해도 0 이야 결국 학습을아무리 해도 반영이 안된다.  Dead ReLu 를 어쩔수 없이 가져가야하는거에요  \n",
    "그래서 Bias term 을 좀 주게 되는거에요 0.01정도 0보다 조금 큰값을 주는 방법으로 약간 이라도 해결이 가능하다.  \n",
    "이런 단점이 있는데 왜쓰는거지? 첫번째 initializing issue 두번째 Dead ReLu가 많이 나도 노드를 엄청 늘려서 죽은게 많아도 살아있는 노드가 많으면 그걸로 할수 있는 거다. ReLu의 문제점에도 불구하고 하는 이유!\n",
    "\n",
    "### ReLU의 변종들\n",
    "\n",
    "ReLu에서 dead ReLU 가 많이 있었잖아 그래서 나온게 Leaky ReLU  \n",
    "하지만 거의우리가 사용하는 거는 ReLU\n",
    "\n",
    "normalization 을 하는 이유? : 변동성을 일치시킨다 그래서 학습시키기가 쉬워진다. 선생님이 그리신 그림을 생각해보자  \n",
    "바이어스텀? : 무조건 있어야한다.\n",
    "팁 : linear하다는 것의 정의 f(m+n) = f(m) + f(n) 이런 것이 linear하다. ( ReLU 를 생각해보면 nonliear 이다 )\n",
    "\n",
    "#### 모멘텀  \n",
    "[수렴하는 방법 총망라](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)\n",
    "\n",
    "\n",
    "gradient descent(batch) vs stochastic graient descent  \n",
    "둘의 차이점? : stochastic 은 보통 확률모델링이들어가는거에요 랜덤 프로세스 데이터를 임의 추출하는 과정이 있을때 stochastic 이라고함  \n",
    "\n",
    "mini batch gradient 데이터 1000개중 100개를 뽑아서 gradient descent 를 날리는거에요 (stochastic)\n",
    "로컬 minimum 을 피할 수 있다!!!! batch를 쓰는 이유 \n",
    "\n",
    "Adagrad Update\n",
    "\n",
    "RMSProp Update\n",
    "\n",
    "Adam Update - (momentum + RMSProp)\n",
    "\n",
    "GD 는 결국 Talyor expansion 으로부터 나온 개념.\n",
    "\n",
    "러닝레이트가 너무크면 에러가 오히려 학습할수록 커져버리는 상황이 일어날 수 있음\n",
    "\n",
    "그렇다고 너무작으면 세월아 네월아 가게된다 즉 러닝레이트도 적당한걸로 !!\n",
    "\n",
    "처음에는 큼직 큼직하게 갔다가 나중에는 세밀하게 하고싶은게 정석\n",
    "\n",
    "그래서 learning rate를 상수로 고정하는거보다 줄여주는거 그림을 보면 learning rate를 높였다 낮췄다 높였다 낮췄다 이지랄 하고있음\n",
    "\n",
    "training accuracy, training loss 이런것만 가지고 판단하면 안됨 당연한 이야기\n",
    "\n",
    "앙상블 학습. - 다수결 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
