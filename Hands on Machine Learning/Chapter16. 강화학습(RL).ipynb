{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16장 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__강화학습(RL)__  \n",
    "강화학습이 무엇인지, 어떤 일을 잘할 수 있는지.  \n",
    "심층강화학습(Deep Reinforcement learning)에서 가장 중요한 두가지 기술인 __정책 그래디언트__ , __심층 Q네트워크(DQN)__을 마르코프 결정과정(MDP)와 함께 설명."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 보상을 최적화하기 위한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습에서 소프트웨어 __에이전트__는 __관측__을 하고 주어진 __환경__에서 __행동__을 한다. 그리고 그 결과로 __보상(reward)__를 받는다.  \n",
    "__에이전트__의 목적은 보상의 장기간 기대치를 최대로 만드는 행동을 학습하는 것이다.  \n",
    "즉, 에이전트는 환경 안에서 행동하고 시행착오를 통해 기쁨이 최대가 되고 아픔이 최소가 되도록 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자율주행 자동차나 웹 페이지에 광고를 배치하는 것, 이미지 분류 시스템이 주의를 집중할 곳을 제어하는 것 등이 강화학습이 잘 들어맞는 사례이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 정책 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트웨어 에이전트가 행동을 결정하기 위해 사용하는 알고리즘을 __정책__ 이라고 한다.  \n",
    "정책은 생각할 수 있는 어떤 알고리즘도 될 수 있으며 결정적일 필요도 없다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들면, 30분 동안 수집한 먼지의 양을 보상으로 받는 로봇 진공청소기를 생각해보면, 이 청소기의 정책은 매 초마다 어떤 확률 p만큼 전진하는 것일 수도 있고, 또는(1-p)확률로 왼쪽 또는 오른쪽으로 랜덤하게 회전하는 것일 수도 있다.  회전 각도는 -r ~ +r 사이의 랜덤 각도일수도 있다.  \n",
    "이런 정책에는 무작위성이 포함되어 있기 때문에 __확률적 정책(Stochastic policy)__이라고한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에는 두개의 __정책 파라미터__ 가 있다. 확률 p와 각도 r인데 생각해볼 수 있는 학습 알고리즘은 이 파라미터에 많은 다른 값을 시도해보고 가장 성능이 좋은 조합을 고르는 것이다. 이것은 __정책 탐색__ 의 예인데 다소 무식한 방법이다.  \n",
    "하지만 __정책 공간__ 이 매우크면 좋은 파라미터 조합을 찾는 것은 매우 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 공간을 탐색하는 방법으로 __유전 알고리즘__ 이 있다.  \n",
    "예를들어 1세대 100개의 정책을 랜덤하게 생성하고.  성능이 낮은 80개의 정책은 버리고, 20개를 살려 각각 4개의 자식 정책을 생산하도록 한다. 이 자식 정책들은 단순히 부모 정책을 복사한후 약간의 무작위성을 더 해준 정책이다.  이렇게 좋은 정책을 선택할때까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방법은 정책 파라미터에 대한 보상의 그래디언트를 평가해서 높은 보상의 방향을 따르는 그래디언트로(즉, __경사 상승법__)으로 파라미터를 수정하는 최적화 기법을 사용하는 것 이다. 이 방법을 __정책 그래디언트(policy gradient, PG)__라고한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 OpenAI 짐(gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 도전과제 중 하나는 에이전트를 훈련시키기 위해 먼저 작업환경을 마련해야 하는 것이다.  \n",
    "즉, 훈련을 위한 최소한의 __시뮬레이션 환경__ 이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OpenAI 짐__ 은 다양한 종류의 시뮬레이션 환경을 제공하는 툴킷이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!md RL\n",
    "\n",
    "!cd RL\n",
    "\n",
    "!pip install --upgrade gym\n",
    "\n",
    "!pip install gym[atari]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 짐(gym)\n",
    "\n",
    "`CartPole-v0`\n",
    "\n",
    "OpenAI Gym의 CartPole은 막대를 판자에서 넘어지지 않도록 __강화학습__을 하는 기본 예제이다.  \n",
    "좌, 우 두개의 행동 공간이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.reset()\n",
    "obs\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    plt.close()  # 이전 그래프를 닫지 않으면 두 개의 그래프가 출력되는 matplotlib의 버그로 보입니다.\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, figsize=(5,6), repeat=False, interval=40):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), \n",
    "                                   frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "try:\n",
    "    from pyglet.gl import gl_info\n",
    "    openai_cart_pole_rendering = True   # 문제없음, OpenAI 짐의 렌더링 함수를 사용합니다\n",
    "except Exception:\n",
    "    openai_cart_pole_rendering = False  # 가능한 X 서버가 없다면, 자체 렌더링 함수를 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_cart_pole(env, obs):\n",
    "    if openai_cart_pole_rendering:\n",
    "        # OpenAI 짐의 렌더링 함수를 사용합니다\n",
    "        return env.render(mode=\"rgb_array\")\n",
    "    else:\n",
    "        # Cart-Pole 환경을 위한 렌더링 (OpenAI 짐이 처리할 수 없는 경우)\n",
    "        img_w = 600\n",
    "        img_h = 400\n",
    "        cart_w = img_w // 12\n",
    "        cart_h = img_h // 15\n",
    "        pole_len = img_h // 3.5\n",
    "        pole_w = img_w // 80 + 1\n",
    "        x_width = 2\n",
    "        max_ang = 0.2\n",
    "        bg_col = (255, 255, 255)\n",
    "        cart_col = 0x000000 # 파랑 초록 빨강\n",
    "        pole_col = 0x669acc # 파랑 초록 빨강\n",
    "\n",
    "        pos, vel, ang, ang_vel = obs\n",
    "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        cart_x = pos * img_w // x_width + img_w // x_width\n",
    "        cart_y = img_h * 95 // 100\n",
    "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
    "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
    "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
    "        draw.rectangle((cart_x - cart_w // 2, cart_y - cart_h // 2, cart_x + cart_w // 2, cart_y + cart_h // 2), fill=cart_col) # draw cart\n",
    "        draw.line((cart_x, cart_y - cart_h // 2, top_pole_x, top_pole_y), fill=pole_col, width=pole_w) # draw pole\n",
    "        return np.array(img)\n",
    "\n",
    "def plot_cart_pole(env, obs):\n",
    "    img = render_cart_pole(env, obs)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABCZJREFUeJzt3VFKAmEUgNGZcBOtw220DluTrqNtuI6WMb1ISGVEWr9+ngOCCgP3QT8uo6PzsiwTAD0PowcA4G8IPECUwANECTxAlMADRAk8QJTAA0QJPECUwANErUYPcOByWoDP5nMOtsEDRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0StRg8Ao+13z18+v95s/3kSuCwbPECUwANECTxAlMADRAk8d8+HqVQJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPJxw6p+e4FYIPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPEzTtN5sR48AFyfwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIP39jvnkePAL8m8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8HCw3mxHjwAXJfAAUQIPECXwAFECDxAl8OTN8/zj218cD6MIPEDUavQAcG1eXjfv958edwMngfPY4OHIcdy/egy3ROABogQeIErg4cjHc+7OwXPL5mVZRs8wTdN0FUPQ9J9fX7yS9xMdZ714bfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFF+Lpg8V5dyr2zwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRq9EDHMyjBwCoscEDRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANEvQGqmiNt/8TBLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_cart_pole(env, obs)\n",
    "\n",
    "env.action_space ##좌, 우 딱 2개의 행동공간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좌로 미는 행동(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 599.5, 399.5, -0.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABIpJREFUeJzt3NFNG1EQQNHdyE2kjlBG6oA2aAPqSBlJHSlj84MSFExkZEW7vnPOF0IY3ge+eszYrNu2LQD0fNr7AAD8HwIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFGnvQ/wwttpAd5ar3mwGzxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0Sd9j4AHMmP54ffH3+5f9rxJHC9ddu2vc+wLMtyiEMw2+u4vyb07Gi95sFGNABRAg8QJfDwD8Yz3DKBB4gSeFjeX7DCLRN4gCiBB4gSeHiHBSu3TuABogSe8SxYqRJ4gCiBhzPM3ykQeIAogQeIEnhGs2ClTOABogQe/mLBSoXAA0QJPECUwDPWuQWr8QwlAg8QJfAAUQIPECXwjOQNTkwg8PDCgpUagQeIEnjGMZ5hCoEHiBJ4gCiBh8WClSaBB4gSeEaxYGUSgQeIEniAKIFnPAtWqgQeIErgGcOClWkEHiBK4AGiBJ7RLFgpE3iAKIFnBAtWJhJ4gCiBB4gSeMayYKVO4AGiBJ48C1amEniAKIEHiBJ4RrJgZQKBB4gSeNLOLVjd3plC4AGiBJ4sL49kOoEHiBJ4gCiBZxQLViYReICo094HgEut63rx135/ur/qe2zbdvHPgqNygweIcoMn69vPP7f4r5+fdzwJ7GM9yJ+ihzgEx/aREc3j4/czn7u7+PEHeV7A5b/0ZxjRAEQJPDnvLVhhGoEn5+7h+c3M3QyeiczguRkfmcFf6yDPCzCDB+AtgQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKP8umJvh3aXwMW7wAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRp70P8GLd+wAANW7wAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUb8AIlVV8TwWhv8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경을 다시 초기화하고 오른쪽으로 미는 행동(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABJRJREFUeJzt3NFNIlEAhlHY0IR1rGVYB7ZhG1LHlqF1bBnsC9kQRR0Enbmf5yQmxoi5L3y5+Udd7/f7FQA9v+Y+AABfQ+ABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIGoz9wEO/DktwGvrS17sBg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDwfPu/u5jwBXtZn7ADAXQafODR6OiD4lAs+P9Xv7OPcR4EsJPLzgFk+FwANECTxAlMDzo9nhKRN4gCiBhxM8aKVA4AGiBJ4fzw5PlcADRAk8vMEOz+gEHiBK4AGiBB5WHrTSJPDwDjs8IxN4gCiBB4gSeDiww1Mj8ABRAg8f8KCVUQk8QJTAwxE7PCUCDxAl8DCBHZ4RCTy8YKahQuBhIrd4RiPwAFECDxAl8HCCHZ4CgYcz2OEZicADRAk8QJTAwxvs8IxO4OFMdnhGIfAAUQIPECXwAFECD+/woJWRCTx8ggetjEDgAaIEHj5gpmFUAg8QJfDwSXZ4lk7gAaIEHiBK4GECD1oZkcDDBezwLJnAw0Ru8YxG4AGiBB4uZKZhqQQeIErg4Qx2eEYi8ABRAg9XYIdniQQeIErgAaIEHs7kQSujEHg4sl6vJ32c8ry7n/z6t34GXJPAA0Rt5j4AjOzP3+3/z+9udjOeBF5b7/f7uc+wWq1WizgEnDOdPDw8nfja7eTXL+S9x7JdtOWZaACiBB6u6Olx+/E3wTcRePikl5v73c1udXtvh2c5bPBw5Dt/fXEh7z2WzQYPwGsCDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABR/l0wHPHXpZS4wQNECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRG3mPsDBeu4DANS4wQNECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0T9A2ReVJyD7rCyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(1)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "plot_cart_pole(env, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "막대가 똑 바로 설 수 있기 위핸 정책(Policy)가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 정책(Policy) 하드코딩\n",
    "\n",
    "frames = []\n",
    "\n",
    "n_max_steps = 1000 #1000번 행동으로 제한\n",
    "n_change_steps = 10 #10번의 행동 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-40723ee45c03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_animation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[1;34m(self, embed_limit)\u001b[0m\n\u001b[0;32m   1337\u001b[0m                 \u001b[1;31m# We create a writer manually so that we can get the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[1;31m# appropriate size for the tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m                 \u001b[0mWriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.writer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m                 writer = Writer(codec='h264',\n\u001b[0;32m   1341\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.bitrate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise RuntimeError(\n\u001b[1;32m--> 164\u001b[1;33m                 'Requested MovieWriter ({}) not available'.format(name))\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "obs = env.reset() #환경 초기화\n",
    "for step in range(n_max_steps):\n",
    "    img = render_cart_pole(env, obs)\n",
    "    frames.append(img)\n",
    "    \n",
    "    #하드코딩 정책\n",
    "    position, velocity, angle, angular_velocity = obs\n",
    "    if angle<0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "        \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "video = plot_animation(frames, figsize=(6,4))\n",
    "HTML(video.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step() 메서드\n",
    "\n",
    "- obs  \n",
    "새로운 관측값. 이제 카트가 오른쪽 방향으로 움직인다(obs[1]>0). 막대가 여전히 오른쪽 방향으로 기울어져있지만(obs[2]>0) 각속도가 음수가 되었으므로(obs[3]<0) 다음 스텝 이후에는 왼쪽으로 기울어질 가능성이 높다.  \n",
    "- reward  \n",
    "이환경에서는 매 스텝마다 무조건 1의 보상을 받는다. 그러므로 시스템의 목적은 가능한 한 오랫동안 실행되는 것이다.  \n",
    "- done  \n",
    "이 값이 True면 에피소드가 끝난 것이다. 막대가 너무 기울어지면 이렇게 된다. 에피소드가 끝나면 환경을 다시 시작하기전에 초기화 해줘야한다.  \n",
    "- info  \n",
    "다른 환경에서는 이 딕셔너리에 추갖거이니 디버깅 정보가 담길 수 있다. 이 데이터는 훈련에 사용하면 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_간단한 정책을 하드 코딩 해보자_  \n",
    "_이 정책은 막대가 왼쪽으로 기울어지면 카트를 왼쪽으로 가속시키고 오른쪽으로 기울어지면 오른쪽으로 가속시킨다. 이 정책으로 500번의 에피소드를 실행해서 얻은 평균 보상을 확인해보자_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2] ##기울어지진 각도에 관측값을 할당. 왜냐면 왼쪽으로 기울어지면 왼쪽, 오른쪽으로 기울어지면 오른쪽으로 가속 시켜야 하므로.\n",
    "    return 0 if angle < 0 else 1 ##각속도가 음수가 된다면 0 아니면 1을 return\n",
    "\n",
    "totals = []\n",
    "\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000): #영원히 실행되면 곤란하므로 최대 스텝을 1000으로 잡는다.\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break;\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.336, 8.58947635190877, 25.0, 71.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500번을 시도해도 이 정책은 막대를 쓰러뜨리지않고 71번보다 많은 스텝을 진행하지는 못했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4 신경망 정책"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0304 16:34:22.623191 25592 deprecation.py:323] From <ipython-input-17-30f27adc7376>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0304 16:34:23.002179 25592 deprecation.py:323] From <ipython-input-17-30f27adc7376>:17: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    }
   ],
   "source": [
    "# 1. 신경망 구조 정의\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # 간단한 문제이므로 많은 히든 뉴런이 필요하지는 않다.\n",
    "n_outputs = 1 #왼쪽으로 가속할 확률만 출력\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "# 2. 신경망 구성\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
    "                        kernel_initializer = initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs,\n",
    "                        kernel_initializer = initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "# 3. 추정된 확률을 기반으로 랜덤하게 행동을 선택\n",
    "p_left_and_right = tf.concat(axis = 1, values = [outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples = 1)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위 신경망 구조에 대한 설명\n",
    "1. 임포트를 하고 난 후 신경망의 구조를 정의합니다. 입력의 개수는 관측 공간의 크기이다.(CartPole경우에는 4). 히든유닛의 개수는 클 필요가 없으므로 4로 지정합니다. 그리고 출력 확률 하나를 가집니다.(왼쪽 방향의 확률)  \n",
    "2. 다음에는 신경망을 구성합니다. 에제에서는 하나의 출력을 가지는 기본적인 다층 퍼셉트론을 사용합니다. 출력층은 0에서 1까지의 호가률을 출력하기위해 로지스틱(시그모이드) 활성화 함수를 사용한다. 만약 가능한 행동이 두개 보다 많으면 행동마다 하나의 출력뉴런을 두고 소프트맥스 확성화 함수를 사용해야 한다.  \n",
    "3. 마지막으로 랜덤하게 행동을 선택하기 위해 multinomial()를 사용한다. 이 함수는 각 정수에 대한 로그 확률이 주어졌을 때 하나의 정수를 독립적으로 선택합니다. 예를 들어 [np.log(0.5), np.log(0.2), np.log(0.3)] 배열과 num_samples = 5로 이 함수를 호출하면 50%의 확률로 0, 20%확률로 1, 30%의 확률로 2를 선택해서 다섯개의 정수를 출력할 것 입니다. 여기에서는 수행할 행동을 나타내는 하나의 정수만 있으면 됩니다. outputs 텐서는 왼쪽으로 갈 확률만 담고 있으므로 먼저 1 - outputs를 연결하여 왼쪽과 오른쪽 방향의 확률을 모두 담은 텐서를 만든다. 가능한 행동이 두 개 이상이면 신경망이 행동마다 하나의 확률을 출력하므로 이렇게 빼서 연결하는 단계는 필요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5 행동 평가:신용 할당 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 스텝에서 가장 좋은 행동이 무엇인지 알고 있다면 추정된 확률과 타깃 확률 사이의 크로스 엔트로피를 최소화하도록 평소처럼 신경망을 훈련시킬 수 있습니다. (지도학습)  \n",
    "하지만 강화학습에서 에이전트가 얻을 수 있는 도움은 __보상(reward)__ 뿐이다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들면 에이전트가 100스텝동안 막대의 균형을 유지했다면 이 100번의 행동중 어떤것이 좋고 나쁜지 알수없다. 우리가 아는것은 마지막 행동 뒤에 막대가 쓰러졌다는 사실 밖에 없다. 하지만 모든 책임이 마지막 행동에 있는 것이 아니다.  \n",
    "이러한 것을 __신용 할다 문제(credit assignment problem)__이라고한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이문제를 해결하기위해서 흔히 사용하는 전략은 행동이 일어난 후 가가 단계마다 __할인 계수__ $\\gamma$를 적용한 보상을 모두 합하여 행동을 평가하는 것 이다.  \n",
    "예를들어, 에이전트가 오른쪽으로 세 번 이동하기로 결정하고, 첫 번째 스텝에서 +10, 두 번째 스텝에서 0, 세 번쨰 스텝에서 -50의 보상을 받았을때 할인 계수 $\\gamma = 0.8$ 일 경우 첫 번째 행동의 전체 점수는 $10 + \\gamma \\times 0 + \\gamma^2 \\times (-50) = -22$ 가 된다.  \n",
    "즉, 할인 계수가 0에 가까우면 미래의 보상은 현재의 보상만큼 중요하게 취급되지 않을 것이다. 반대로 할인 계수가 1에 가까우면 미래의 보상은 현재의 보상만큼 중요하게 고려될 것이다.  \n",
    "CartPole의 경우 행동의 효과가 매우 짧은 기간 안에 나타나므로 할인 계수 0.95가 적절하다.   \n",
    "좋은 행동 뒤에 나쁜 행동이 몇 번 뒤따르면 막대가 금방 넘어질 것이므로 좋은 행동이 낮은 점수를 얻을 것이다.  \n",
    "그러나 게임을 충분히 많은 횟수만큼 반복하면 평균적으로 좋은 행동이 나쁜 행동보다 더 높은 점수를 가질 것이고. 반드시 모든 행동의 점수를 정규화 해야한다. 이렇게해야 음수인 행동은 나쁜 행동이고 양수인 행동은 좋은 행동이라고 구별 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6 정책 그래디언트"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
