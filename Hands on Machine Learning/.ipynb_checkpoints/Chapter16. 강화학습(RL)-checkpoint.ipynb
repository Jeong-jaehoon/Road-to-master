{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16장 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__강화학습(RL)__  \n",
    "강화학습이 무엇인지, 어떤 일을 잘할 수 있는지.  \n",
    "심층강화학습(Deep Reinforcement learning)에서 가장 중요한 두가지 기술인 __정책 그래디언트__ , __심층 Q네트워크(DQN)__을 마르코프 결정과정(MDP)와 함께 설명."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 보상을 최적화하기 위한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습에서 소프트웨어 __에이전트__는 __관측__을 하고 주어진 __환경__에서 __행동__을 한다. 그리고 그 결과로 __보상(reward)__를 받는다.  \n",
    "__에이전트__의 목적은 보상의 장기간 기대치를 최대로 만드는 행동을 학습하는 것이다.  \n",
    "즉, 에이전트는 환경 안에서 행동하고 시행착오를 통해 기쁨이 최대가 되고 아픔이 최소가 되도록 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자율주행 자동차나 웹 페이지에 광고를 배치하는 것, 이미지 분류 시스템이 주의를 집중할 곳을 제어하는 것 등이 강화학습이 잘 들어맞는 사례이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 정책 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트웨어 에이전트가 행동을 결정하기 위해 사용하는 알고리즘을 __정책__ 이라고 한다.  \n",
    "정책은 생각할 수 있는 어떤 알고리즘도 될 수 있으며 결정적일 필요도 없다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들면, 30분 동안 수집한 먼지의 양을 보상으로 받는 로봇 진공청소기를 생각해보면, 이 청소기의 정책은 매 초마다 어떤 확률 p만큼 전진하는 것일 수도 있고, 또는(1-p)확률로 왼쪽 또는 오른쪽으로 랜덤하게 회전하는 것일 수도 있다.  회전 각도는 -r ~ +r 사이의 랜덤 각도일수도 있다.  \n",
    "이런 정책에는 무작위성이 포함되어 있기 때문에 __확률적 정책(Stochastic policy)__이라고한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에는 두개의 __정책 파라미터__ 가 있다. 확률 p와 각도 r인데 생각해볼 수 있는 학습 알고리즘은 이 파라미터에 많은 다른 값을 시도해보고 가장 성능이 좋은 조합을 고르는 것이다. 이것은 __정책 탐색__ 의 예인데 다소 무식한 방법이다.  \n",
    "하지만 __정책 공간__ 이 매우크면 좋은 파라미터 조합을 찾는 것은 매우 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 공간을 탐색하는 방법으로 __유전 알고리즘__ 이 있다.  \n",
    "예를들어 1세대 100개의 정책을 랜덤하게 생성하고.  성능이 낮은 80개의 정책은 버리고, 20개를 살려 각각 4개의 자식 정책을 생산하도록 한다. 이 자식 정책들은 단순히 부모 정책을 복사한후 약간의 무작위성을 더 해준 정책이다.  이렇게 좋은 정책을 선택할때까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방법은 정책 파라미터에 대한 보상의 그래디언트를 평가해서 높은 보상의 방향을 따르는 그래디언트로(즉, __경사 상승법__)으로 파라미터를 수정하는 최적화 기법을 사용하는 것 이다. 이 방법을 __정책 그래디언트(policy gradient, PG)__라고한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 OpenAI 짐(gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 도전과제 중 하나는 에이전트를 훈련시키기 위해 먼저 작업환경을 마련해야 하는 것이다.  \n",
    "즉, 훈련을 위한 최소한의 __시뮬레이션 환경__ 이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OpenAI 짐__ 은 다양한 종류의 시뮬레이션 환경을 제공하는 툴킷이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!md RL\n",
    "\n",
    "!cd RL\n",
    "\n",
    "!pip install --upgrade gym\n",
    "\n",
    "!pip install gym[atari]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 짐(gym)\n",
    "\n",
    "`CartPole-v0`\n",
    "\n",
    "OpenAI Gym의 CartPole은 막대를 판자에서 넘어지지 않도록 __강화학습__을 하는 기본 예제이다.  \n",
    "좌, 우 두개의 행동 공간이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.reset()\n",
    "obs\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    plt.close()  # 이전 그래프를 닫지 않으면 두 개의 그래프가 출력되는 matplotlib의 버그로 보입니다.\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, figsize=(5,6), repeat=False, interval=40):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), \n",
    "                                   frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "try:\n",
    "    from pyglet.gl import gl_info\n",
    "    openai_cart_pole_rendering = True   # 문제없음, OpenAI 짐의 렌더링 함수를 사용합니다\n",
    "except Exception:\n",
    "    openai_cart_pole_rendering = False  # 가능한 X 서버가 없다면, 자체 렌더링 함수를 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_cart_pole(env, obs):\n",
    "    if openai_cart_pole_rendering:\n",
    "        # OpenAI 짐의 렌더링 함수를 사용합니다\n",
    "        return env.render(mode=\"rgb_array\")\n",
    "    else:\n",
    "        # Cart-Pole 환경을 위한 렌더링 (OpenAI 짐이 처리할 수 없는 경우)\n",
    "        img_w = 600\n",
    "        img_h = 400\n",
    "        cart_w = img_w // 12\n",
    "        cart_h = img_h // 15\n",
    "        pole_len = img_h // 3.5\n",
    "        pole_w = img_w // 80 + 1\n",
    "        x_width = 2\n",
    "        max_ang = 0.2\n",
    "        bg_col = (255, 255, 255)\n",
    "        cart_col = 0x000000 # 파랑 초록 빨강\n",
    "        pole_col = 0x669acc # 파랑 초록 빨강\n",
    "\n",
    "        pos, vel, ang, ang_vel = obs\n",
    "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        cart_x = pos * img_w // x_width + img_w // x_width\n",
    "        cart_y = img_h * 95 // 100\n",
    "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
    "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
    "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
    "        draw.rectangle((cart_x - cart_w // 2, cart_y - cart_h // 2, cart_x + cart_w // 2, cart_y + cart_h // 2), fill=cart_col) # draw cart\n",
    "        draw.line((cart_x, cart_y - cart_h // 2, top_pole_x, top_pole_y), fill=pole_col, width=pole_w) # draw pole\n",
    "        return np.array(img)\n",
    "\n",
    "def plot_cart_pole(env, obs):\n",
    "    img = render_cart_pole(env, obs)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABAxJREFUeJzt3MFpAlEARdE/YZpIHWkjdWhNsY60YR0pY7Jxk4gQUPPNzTngQkF5C718xHHZtm0A0PM0ewAA9yHwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxC1zh5w4nJagHPLNU92ggeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiFpnD4BHcTzszx572b1NWAK34QQPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8DDGOB72syfAzQk8QJTAA0QJPFnLsvz4ds/XgFkEHiBqnT0AHsX7x+7L/dfnw6QlcBtO8HDB9+DDXyPwMMScJoGH4esYmgQeLhB9/rpl27bZG8YY4yFG0PLbP118kM8SLVe9iZ3gAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGi/F0wWa4s5b9zggeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiFpnDzhZZg8AqHGCB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiPoE3xQeCVZD9OIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_cart_pole(env, obs)\n",
    "\n",
    "env.action_space ##좌, 우 딱 2개의 행동공간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좌로 미는 행동(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 599.5, 399.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABINJREFUeJzt3M1NG1EUgNE3EU2kjqSM1AFtuI1QR8oIdaSMyYZFwk8iZMHY3z1HYgGS0V2YT0/3jdn2fV8A9Hw6egAA3ofAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPEDUzdEDPPJxWoDntnNe7AQPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg9PPNzfrYf7u6PHgLNt+74fPcNaa13EEPBa2L/cfv/gSWCttdZ2zoud4AGiBB4gSuDhP6xnuFYCDxAl8PDIkzPUCDxAlMADRAk8/IMLVq6ZwMOyf6dJ4AGiBB4gSuDhFfbvXDuBB4gSeMZzwUqVwANECTy8wP6dAoEHiBJ4RrN/p0zgAaIEHiBK4OEJF6xUCDxj2b9TJ/DwB6d3SgQeIErgAaIEHiBK4BnppQtW+3dqBB4gSuAZx+ORTCHwAFECDxAl8LBcsNIk8Ixi/84kAg8QJfAAUQLPePbvVAk8QJTAM4YLVqYReIAogQeIEnhGc8FKmcAzgv07Ewk8QJTAA0QJPGPZv1Mn8ABRAk+eC1amEniAKIEHiBJ4RnLBygQCT5r9O5MJPOM4vTOFwJPl9M50Ag8QJfAAUQLPKPbvTCLwAFECT5ILVhB4rsS2bW/6Ovf3QIHAM8bXu/ujR4APdXP0APAefvy6/ev7b5/FnXmc4Mk5nX4++9nT4MMEAg8QJfAAUQJPzkv79tPp6wGTwLG2fd+PnmGttS5iCC7XRz+6eCF/F3DWG98JHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gyr8L5ir4ZCm8nRM8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPEDUzdEDPNqOHgCgxgkeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4g6jctxk8GYffJ0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경을 다시 초기화하고 오른쪽으로 미는 행동(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABJFJREFUeJzt3M1tE1EUgNEZlCaoA8qgjqSNtIHroAxSB2UMGy/AcRL/wcz7fI7EBsnoLsinp3uTzMuyTAD0fFp7AAD+DYEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gKiHtQfY8+O0AK/N13zYCx4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHqZpetk9TS+7p7XHgJual2VZe4ZpmqZNDMH9ORb1L4/fV5gEjpqv+bAXPECUwMMBqxoqBB4gSuABogSeu+agSpnAA0QJPBzh0EqBwANECTxAlMBz9xxaqRJ4eIM9PKMTeIAogQeIEniY7OFpEniAKIGHdzi0MjKBB4gSeIAogYc9h1ZqBB4gSuDhAw6tjErgAaIEHv5gD0+JwANECTycwB6eEQk8HLCmoULg4URe8YxG4AGiBB4gSuDhCHt4CgQeIErg4QwOrYxE4AGiBB4gSuDhDQ6tjE7g4Uz28IxC4AGiBB4gSuDhHfbwjEzgAaIEHi7g0MoIBB4gSuABogQePuDQyqgEHiBK4OFCDq1sncADRAk8nMAenhEJPECUwMMV7OHZMoGHE1nTMBqBhyt5xbNVAg8QJfAAUQIPECXwcAaHVkYi8HADDq1skcADRAk87M3zfNKfaz//0b8DtyLwcKavT7u1R4CTPKw9AIzqx6/Hg78RfrbFCx5u5Pn559ojwF8EHi7w+vUO2yPwcIFvn61j2D6BhwscO7SKPlszL8uy9gzTNE2bGIL79r+/dXEjX3ts21X/Kb3gAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGi/Lpg2POTpdR4wQNECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRD2sPcDevPYAADVe8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFG/Aew7Tm8o7gSkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(1)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "plot_cart_pole(env, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "막대가 똑 바로 설 수 있기 위핸 정책(Policy)가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 정책(Policy) 하드코딩\n",
    "\n",
    "frames = []\n",
    "\n",
    "n_max_steps = 1000 #1000번 행동으로 제한\n",
    "n_change_steps = 10 #10번의 행동 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-40723ee45c03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_animation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[1;34m(self, embed_limit)\u001b[0m\n\u001b[0;32m   1337\u001b[0m                 \u001b[1;31m# We create a writer manually so that we can get the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[1;31m# appropriate size for the tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m                 \u001b[0mWriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.writer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m                 writer = Writer(codec='h264',\n\u001b[0;32m   1341\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.bitrate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise RuntimeError(\n\u001b[1;32m--> 164\u001b[1;33m                 'Requested MovieWriter ({}) not available'.format(name))\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "# obs = env.reset() #환경 초기화\n",
    "# for step in range(n_max_steps):\n",
    "#     img = render_cart_pole(env, obs)\n",
    "#     frames.append(img)\n",
    "    \n",
    "#     #하드코딩 정책\n",
    "#     position, velocity, angle, angular_velocity = obs\n",
    "#     if angle<0:\n",
    "#         action = 0\n",
    "#     else:\n",
    "#         action = 1\n",
    "        \n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# video = plot_animation(frames, figsize=(6,4))\n",
    "# HTML(video.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step() 메서드\n",
    "\n",
    "- obs  \n",
    "새로운 관측값. 이제 카트가 오른쪽 방향으로 움직인다(obs[1]>0). 막대가 여전히 오른쪽 방향으로 기울어져있지만(obs[2]>0) 각속도가 음수가 되었으므로(obs[3]<0) 다음 스텝 이후에는 왼쪽으로 기울어질 가능성이 높다.  \n",
    "- reward  \n",
    "이환경에서는 매 스텝마다 무조건 1의 보상을 받는다. 그러므로 시스템의 목적은 가능한 한 오랫동안 실행되는 것이다.  \n",
    "- done  \n",
    "이 값이 True면 에피소드가 끝난 것이다. 막대가 너무 기울어지면 이렇게 된다. 에피소드가 끝나면 환경을 다시 시작하기전에 초기화 해줘야한다.  \n",
    "- info  \n",
    "다른 환경에서는 이 딕셔너리에 추갖거이니 디버깅 정보가 담길 수 있다. 이 데이터는 훈련에 사용하면 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_간단한 정책을 하드 코딩 해보자_  \n",
    "_이 정책은 막대가 왼쪽으로 기울어지면 카트를 왼쪽으로 가속시키고 오른쪽으로 기울어지면 오른쪽으로 가속시킨다. 이 정책으로 500번의 에피소드를 실행해서 얻은 평균 보상을 확인해보자_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2] ##기울어지진 각도에 관측값을 할당. 왜냐면 왼쪽으로 기울어지면 왼쪽, 오른쪽으로 기울어지면 오른쪽으로 가속 시켜야 하므로.\n",
    "    return 0 if angle < 0 else 1 ##각속도가 음수가 된다면 0 아니면 1을 return\n",
    "\n",
    "totals = []\n",
    "\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000): #영원히 실행되면 곤란하므로 최대 스텝을 1000으로 잡는다.\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break;\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.664, 8.317878575694646, 24.0, 68.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500번을 시도해도 이 정책은 막대를 쓰러뜨리지않고 71번보다 많은 스텝을 진행하지는 못했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4 신경망 정책"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0305 14:06:04.596869 19044 deprecation.py:323] From <ipython-input-13-30f27adc7376>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0305 14:06:04.941917 19044 deprecation.py:323] From <ipython-input-13-30f27adc7376>:17: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    }
   ],
   "source": [
    "# 1. 신경망 구조 정의\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # 간단한 문제이므로 많은 히든 뉴런이 필요하지는 않다.\n",
    "n_outputs = 1 #왼쪽으로 가속할 확률만 출력\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "# 2. 신경망 구성\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
    "                        kernel_initializer = initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs,\n",
    "                        kernel_initializer = initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "# 3. 추정된 확률을 기반으로 랜덤하게 행동을 선택\n",
    "p_left_and_right = tf.concat(axis = 1, values = [outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples = 1)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위 신경망 구조에 대한 설명\n",
    "1. 임포트를 하고 난 후 신경망의 구조를 정의합니다. 입력의 개수는 관측 공간의 크기이다.(CartPole경우에는 4). 히든유닛의 개수는 클 필요가 없으므로 4로 지정합니다. 그리고 출력 확률 하나를 가집니다.(왼쪽 방향의 확률)  \n",
    "2. 다음에는 신경망을 구성합니다. 에제에서는 하나의 출력을 가지는 기본적인 다층 퍼셉트론을 사용합니다. 출력층은 0에서 1까지의 호가률을 출력하기위해 로지스틱(시그모이드) 활성화 함수를 사용한다. 만약 가능한 행동이 두개 보다 많으면 행동마다 하나의 출력뉴런을 두고 소프트맥스 확성화 함수를 사용해야 한다.  \n",
    "3. 마지막으로 랜덤하게 행동을 선택하기 위해 multinomial()를 사용한다. 이 함수는 각 정수에 대한 로그 확률이 주어졌을 때 하나의 정수를 독립적으로 선택합니다. 예를 들어 [np.log(0.5), np.log(0.2), np.log(0.3)] 배열과 num_samples = 5로 이 함수를 호출하면 50%의 확률로 0, 20%확률로 1, 30%의 확률로 2를 선택해서 다섯개의 정수를 출력할 것 입니다. 여기에서는 수행할 행동을 나타내는 하나의 정수만 있으면 됩니다. outputs 텐서는 왼쪽으로 갈 확률만 담고 있으므로 먼저 1 - outputs를 연결하여 왼쪽과 오른쪽 방향의 확률을 모두 담은 텐서를 만든다. 가능한 행동이 두 개 이상이면 신경망이 행동마다 하나의 확률을 출력하므로 이렇게 빼서 연결하는 단계는 필요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5 행동 평가:신용 할당 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 스텝에서 가장 좋은 행동이 무엇인지 알고 있다면 추정된 확률과 타깃 확률 사이의 크로스 엔트로피를 최소화하도록 평소처럼 신경망을 훈련시킬 수 있습니다. (지도학습)  \n",
    "하지만 강화학습에서 에이전트가 얻을 수 있는 도움은 __보상(reward)__ 뿐이다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들면 에이전트가 100스텝동안 막대의 균형을 유지했다면 이 100번의 행동중 어떤것이 좋고 나쁜지 알수없다. 우리가 아는것은 마지막 행동 뒤에 막대가 쓰러졌다는 사실 밖에 없다. 하지만 모든 책임이 마지막 행동에 있는 것이 아니다.  \n",
    "이러한 것을 __신용 할다 문제(credit assignment problem)__이라고한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이문제를 해결하기위해서 흔히 사용하는 전략은 행동이 일어난 후 가가 단계마다 __할인 계수__ $\\gamma$를 적용한 보상을 모두 합하여 행동을 평가하는 것 이다.  \n",
    "예를들어, 에이전트가 오른쪽으로 세 번 이동하기로 결정하고, 첫 번째 스텝에서 +10, 두 번째 스텝에서 0, 세 번쨰 스텝에서 -50의 보상을 받았을때 할인 계수 $\\gamma = 0.8$ 일 경우 첫 번째 행동의 전체 점수는 $10 + \\gamma \\times 0 + \\gamma^2 \\times (-50) = -22$ 가 된다.  \n",
    "즉, 할인 계수가 0에 가까우면 미래의 보상은 현재의 보상만큼 중요하게 취급되지 않을 것이다. 반대로 할인 계수가 1에 가까우면 미래의 보상은 현재의 보상만큼 중요하게 고려될 것이다.  \n",
    "CartPole의 경우 행동의 효과가 매우 짧은 기간 안에 나타나므로 할인 계수 0.95가 적절하다.   \n",
    "좋은 행동 뒤에 나쁜 행동이 몇 번 뒤따르면 막대가 금방 넘어질 것이므로 좋은 행동이 낮은 점수를 얻을 것이다.  \n",
    "그러나 게임을 충분히 많은 횟수만큼 반복하면 평균적으로 좋은 행동이 나쁜 행동보다 더 높은 점수를 가질 것이고. 반드시 모든 행동의 점수를 정규화 해야한다. 이렇게해야 음수인 행동은 나쁜 행동이고 양수인 행동은 좋은 행동이라고 구별 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6 정책 그래디언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PG알고리즘 (policy gradient)은 높은 보상을 얻는 방향의 그래디언트로 정책의 파라미터를 최적화 하는 알고리즘이다. 대표적인 PG 알고리즘은 로날드 윌리엄스의 Reinforce 알고리즘이다.  \n",
    "1. 신경망 정책이 여러 번에 걸쳐 게임을 플레이하고 매 스텝마다 선택된 행동이 더 높은 가능성을 가지도록 만드는 그래디언트를 계산한다. 하지만 아직 이 그래디언트를 적용하지는 않는다.\n",
    "2. 몇 번의 에피소드를 실행한 다음, 각 행동의 점수를 계산한다. \n",
    "3. 한 행동의 점수가 양수면 이 행동이 좋은 것임을 의미하므로 미래에 선택될 가능성이 높도록 앞서 계산한 그래디언트를 적용한다. 그러나 점수가 음수면 이 행동이 나쁜 것임을 의미하므로 미래에 이 해동이 덜 선택 되도록 반대의 그래디언트를 적용합니다. 이는 각 그래디언트 벡터와 그에 상응하는 행동의 점수를 곱함현 된다.\n",
    "4. 마지막으로 모든 결과 그래디언트 벡터를 평균내어 경사 하강법 스텝을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서 플로우로 위 알고리즘을 구현해보자.  \n",
    "선택된 행동이 0(왼쪽) 이면 타깃 확률은 1이고 1(오른쪽)이면 타깃 확률은 0이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0305 14:06:04.954882 19044 deprecation.py:323] From <ipython-input-14-8f8dde413370>:1: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "y = 1. - tf.to_float(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer에 minimize() 메서드대신 comput_gradients 메서드를 사용.  \n",
    "-> 그래디언트를 사용하기전에 조금 수정하기 위함.  \n",
    "compute_gradients는 그래디언트 벡터/변수 쌍의 리스트를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = [grad for grad, variable in grads_and_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와같이 gradients에 모든 그래디언트를 리스트에 담자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape = grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder,variable))\n",
    "    \n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PG 전체 구성 단계 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetype error -> 이유를 알 수 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)  # 행동 0(왼쪽)에 대한 확률\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "y = 1. - tf.cast(action, float)\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "grads_and_vars = grads_and_vars[-4:] ##뒤에서부터 4개 빼주기\n",
    "\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 에피소드에 걸친 결과를 정규화하는 두개의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 정책을 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 250 # 훈련 반복 횟수\n",
    "n_max_steps = 1000 # 에피소드별 최대 스텝\n",
    "n_games_per_update = 10 #10번의 에피소드마다 정책을 훈련\n",
    "save_iterations = 10 #10번의 훈련마다 모델을 저장\n",
    "discount_factor = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards = [] # 각 에피소드에 대한 모든 원본 보상의 리스트\n",
    "        all_gradients = [] # 각 에피소드의 스텝마다 저장된 그레디언트\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = [] # 현재 에피소드에서 얻은 모든 원본 보상\n",
    "            current_gradients = [] # 현재 에피소드에서 얻은 모든 그레디언트\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps): #최대 1000번 실행\n",
    "                action_val, gradients_val = sess.run(\n",
    "                    [action, gradients],\n",
    "                    feed_dict = {X: obs.reshape(1, n_inputs)}) #한 개의 관측\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        # 여기까지 10번의 에피소드에서 정책을 실행한것\n",
    "        \n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            # 그래디언트와 행동 점수를 곱하고 평균을 계산\n",
    "            mean_gradients = np.mean(\n",
    "            [reward * all_gradients[game_index][step][var_index]\n",
    "                for game_index, rewards in enumerate(all_rewards)\n",
    "                for step, reward in enumerate(rewards)], axis = 0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict = feed_dict)\n",
    "        if iteration % save_iterations ==0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파고는 이와 비슷한 PG알고리즘(몬테칼로 트리 검색) 을 기반으로 했다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7 마르코프 결정 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[마르코프 연쇄] https://ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쉽게 말해서 상태 s에서 s' 로 전이되는 확률은 고정되어있으며 과거 상태와 상관없이 (s,s')의 쌍에만 의존한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 마르코프 연쇄 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상태: 0 0 0 0 0 1 2 1 2 1 2 1 3 \n",
      "상태: 0 0 0 0 3 \n",
      "상태: 0 0 0 0 0 0 3 \n",
      "상태: 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 ...\n",
      "상태: 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "상태: 0 1 3 \n",
      "상태: 0 3 \n",
      "상태: 0 0 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "상태: 0 0 0 0 0 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "상태: 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 ...\n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [0.7, 0.2, 0.0, 0.1],  # s0에서 s0, s1, s2, s3으로\n",
    "        [0.0, 0.0, 0.9, 0.1],  # s1에서 ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # s2에서 ...\n",
    "        [0.0, 0.0, 0.0, 1.0],  # s3에서 ...\n",
    "    ]\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence(start_state=0):\n",
    "    current_state = start_state\n",
    "    print(\"상태:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 마르코프 결정 과정 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_fire\n",
      "상태 (+보상): 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 1 (-50) 2 (40) ... 전체 보상 = 50\n",
      "상태 (+보상): 0 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 1 (-50) 2 (40) 0 (10) ... 전체 보상 = 160\n",
      "상태 (+보상): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 2 (40) 0 (10) ... 전체 보상 = 210\n",
      "상태 (+보상): 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) ... 전체 보상 = -10\n",
      "상태 (+보상): 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 1 (-50) ... 전체 보상 = 20\n",
      "요약: 평균=115.1, 표준 편차=131.895375, 최소=-360, 최대=500\n",
      "\n",
      "policy_random\n",
      "상태 (+보상): 0 0 (10) 0 0 0 1 1 1 1 (-50) 2 (40) ... 전체 보상 = 20\n",
      "상태 (+보상): 0 (10) 0 0 (10) 0 0 1 1 1 1 (-50) 2 (40) ... 전체 보상 = 100\n",
      "상태 (+보상): 0 0 1 1 1 (-50) 2 1 1 1 1 ... 전체 보상 = -160\n",
      "상태 (+보상): 0 1 1 1 1 (-50) 2 (40) 0 0 0 0 ... 전체 보상 = -90\n",
      "상태 (+보상): 0 1 1 1 1 (-50) 2 (40) 0 1 (-50) 2 1 (-50) ... 전체 보상 = -80\n",
      "요약: 평균=-23.2, 표준 편차=89.995013, 최소=-280, 최대=240\n",
      "\n",
      "policy_safe\n",
      "상태 (+보상): 0 (10) 0 1 1 1 1 1 1 1 1 ... 전체 보상 = 10\n",
      "상태 (+보상): 0 1 1 1 1 1 1 1 1 1 ... 전체 보상 = 0\n",
      "상태 (+보상): 0 (10) 0 1 1 1 1 1 1 1 1 ... 전체 보상 = 10\n",
      "상태 (+보상): 0 (10) 0 (10) 0 (10) 0 1 1 1 1 1 1 ... 전체 보상 = 30\n",
      "상태 (+보상): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... 전체 보상 = 100\n",
      "요약: 평균=23.7, 표준 편차=28.167435, 최소=0, 최대=180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], # s0에서, 행동 a0이 선택되면 0.7의 확률로 상태 s0로 가고 0.3의 확률로 상태 s1로 가는 식입니다.\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None],\n",
    "    ]\n",
    "\n",
    "rewards = [\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
    "    ]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "def policy_fire(state):\n",
    "    return [0, 2, 1][state]\n",
    "\n",
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "def policy_safe(state):\n",
    "    return [0, 0, 1][state]\n",
    "\n",
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state=start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward\n",
    "\n",
    "def run_episode(policy, n_steps, start_state=0, display=True):\n",
    "    env = MDPEnvironment()\n",
    "    if display:\n",
    "        print(\"상태 (+보상):\", end=\" \")\n",
    "    for step in range(n_steps):\n",
    "        if display:\n",
    "            if step == 10:\n",
    "                print(\"...\", end=\" \")\n",
    "            elif step < 10:\n",
    "                print(env.state, end=\" \")\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "        if display and step < 10:\n",
    "            if reward:\n",
    "                print(\"({})\".format(reward), end=\" \")\n",
    "    if display:\n",
    "        print(\"전체 보상 =\", env.total_rewards)\n",
    "    return env.total_rewards\n",
    "\n",
    "for policy in (policy_fire, policy_random, policy_safe):\n",
    "    all_totals = []\n",
    "    print(policy.__name__)\n",
    "    for episode in range(1000):\n",
    "        all_totals.append(run_episode(policy, n_steps=100, display=(episode<5)))\n",
    "    print(\"요약: 평균={:.1f}, 표준 편차={:1f}, 최소={}, 최대={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습에서는 마르코프 연쇄를 어떻게 사용할까?  \n",
    "각 상태로 전이하는데 특정 확률로 다른상태로 이동할때 양의 보상 또는 음의 보상을 심어 준다음  \n",
    "벨만의 최적 상태 가치(state value) $V^*(s)$ 를 추정하는 __벨만 최적 방정식__을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 식이 의미하는 것은 에이전트가 최적으로 행동하면 현재 상태의 최적 가치는 하나의 최적 행동으로 인해 평균적으로 받게 될 보상과 이 행동이 유발 할 수 있는 가능한 모든 다음 상태의 최적 가치의 기대치를 합한것과 같다는 것이다.  \n",
    "모든 가치 상태를 0으로 초기화 한후 가치 반복(Value Iteration)을 반복적으로 업데이트하여 충분한 반복을 한다면 이 추정값이 최적의 정책에 대응되는 최적의 상태 가치에 수렴하는 것이 보장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note`  \n",
    "이 알고리즘은 __동적 계획법(Dynamic programming)__ 의 한 예로서 복잡한 문제를 다루기 쉬운 하위문제로 나누어 반복적으로 해결한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적의 상태 가치를 아는 것은 특히 정책을 평가할 때 유용하다. 하지만 에이전트가 해야 할 일을 알려주지는 않는다. \n",
    "다행히 벨만은 __Q가치__ 라고 부르는 최적의 __상태-행동 가치__ 를 추정할 수 있는 매우 비슷한 알고리즘을 발견했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 알고리즘을 책의 572페이지 그림 16-8 도식에 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-가치 반복 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan #불가능한 행동을 의미\n",
    "T = np.array([ # 크기 = [s, a, s`]\n",
    "[[0.7,0.3,0.0], [1.0,0.0,0.0], [0.8,0.2,0.0]],\n",
    "[[0.0,1.0,0.0], [nan,nan,nan], [0.0,0.0,1.0]],\n",
    "[[nan,nan,nan], [0.8,0.1,0.1], [nan,nan,nan]],\n",
    "])\n",
    "\n",
    "R = np.array([ # 크기 = [s, a, s`]\n",
    "[[10.,0.0,0.0], [0.0,0.0,0.0], [0.0,0.0,0.0]],\n",
    "[[0.0,0.0,0.0], [nan,nan,nan], [0.0,0.0,-50.]],\n",
    "[[nan,nan,nan], [40.,0.0,0.0], [nan,nan,nan]],\n",
    "])\n",
    "possible_actions = [[0,1,2], [0,2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7, 0.3, 0. ],\n",
       "        [1. , 0. , 0. ],\n",
       "        [0.8, 0.2, 0. ]],\n",
       "\n",
       "       [[0. , 1. , 0. ],\n",
       "        [nan, nan, nan],\n",
       "        [0. , 0. , 1. ]],\n",
       "\n",
       "       [[nan, nan, nan],\n",
       "        [0.8, 0.1, 0.1],\n",
       "        [nan, nan, nan]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.full((3,3), -np.inf) #불가능한 행동에 대해서는 -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-inf, -inf, -inf],\n",
       "       [-inf, -inf, -inf],\n",
       "       [-inf, -inf, -inf]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.]\n",
      " [-inf -inf -inf]\n",
      " [-inf -inf -inf]]\n",
      "[[ nan  nan  nan]\n",
      " [  0. -inf   0.]\n",
      " [-inf  nan -inf]]\n",
      "[[ nan  nan  nan]\n",
      " [ nan -inf  nan]\n",
      " [-inf   0. -inf]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0\n",
    "    discount_factor = 0.95\n",
    "    n_iterations = 100\n",
    "    print(Q)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        Q_prev = Q.copy()\n",
    "        for s in range(3):\n",
    "            for a in possible_actions[s]:\n",
    "                Q[s, a] = np.sum([\n",
    "                    T[s,a,sp] * (R[s, a, sp] + discount_factor * np.max(Q_prev[sp]))\n",
    "                    for sp in range(3)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan],\n",
       "       [ nan, -inf,  nan],\n",
       "       [-inf,  nan, -inf]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
